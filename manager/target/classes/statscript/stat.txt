### 统计 HTTP状态码 escape |(vertical bar) using \\| : https://www.gnu.org/software/gawk/manual/html_node/Escape-Sequences.html
grep "crawlerResult" spider-schedule-stat.log | awk -F ' -\\|- ' '{print  $5}' | awk -F " : " '{count[$2]++}END{for(var in count){print var " " count[var]}}'

###统计　抽取超时页面：
grep "crawlerResult" spider-schedule-stat.log | grep "timeout" -c

###统计　抽取失败：
grep "extract" spider-schedule-stat.log  | grep "status : Failure" -c

##抽取成功
grep "extract" spider-schedule-stat.log  | grep "status : Success" -c

###抽取成功　不合法的页面
grep "extract" spider-schedule-stat.log  | grep "status : Success" | grep "isValid=false" -c

###延迟分布
系统产出的文章总量、文章的时延分布（与源文时间对比）、站点分布、抽取质量等
1.文章总量
mysql 
newsDay=`date +%Y-%m-%d`
thisDay=`date -d "-1 day" +%Y-%m-%d`
mysql -h 10.13.82.17 -u recom -pxsKR6QSufx --default-character-set=utf8 -s -e "select count(*) from spider.news_index where createTime>='$newsDay' and createTime<'$thisDay';"

2.文章的时延分布
delayTime:

300以内
600以内
1200以内
2400以内
3600以内
7200以内
14400以内
28800以内
36000以内
36000以上 输出到日志文件中

grep "extract" spider-schedule-stat.log | grep "Success" | grep DETAIL | grep "domain : " | awk -F ' -\\|- ' -v efile="error.txt" 'BEGIN{OFS=" : ";maxDelay=36000;delayRange[1]=0;delayRange[2]=300;delayRange[3]=600;delayRange[4]=1200;delayRange[5]=2400;delayRange[6]=3600;delayRange[7]=7200;delayRange[8]=14400;delayRange[9]=28800;delayRange[10]=36000;}{split($9,dArray," : ");value=dArray[2];if(value>=maxDelay){delays[maxDelay*10]++;print($4,$9) > efile}else{for(i=1;i<=length(delayRange);i++){key=delayRange[i];if(value<key){delays[key]++;break;}}}}END{for(delay in delays){print delay,delays[delay]}}' | sort -t : -k 1,1 -n 


3.站点分布
domain --- number
grep "extract" spider-schedule-stat.log | grep "Success" | grep DETAIL | grep "domain : " | awk -F ' -\\|- ' 'BEGIN{OFS=" : ";}{split($7,domainArray," : ");domains[domainArray[2]]++;}END{print "domain distribution : ";for(domain in domains){print domain,domains[domain]};}' | sort -t : -k 2,2 -nr


domain associate array

4.抽取质量